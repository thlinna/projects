// File: specs/agent_definitions.pseudo
// Version: 1.0
// Description: Defines the base structure and specific roles for each AI agent.

MODULE AgentDefinitions

  // --- Dependencies ---
  IMPORT LLM_Service // Interface to the underlying Large Language Model (local or cloud)
  IMPORT WebBrowser // Interface for limited web browsing capability
  IMPORT LocalDatabase // To access historical data or context if needed

  // --- Base Agent Structure ---
  DEFINE ABSTRACT STRUCTURE Agent:
    role AS STRING
    llm AS LLM_Service
    browser AS WebBrowser (Optional)
    db AS LocalDatabase

    // Abstract method to be implemented by each specific agent
    ABSTRACT FUNCTION execute_task(task_details, project_context) RETURNS AgentResult
  END STRUCTURE

  // --- Agent Result Structure ---
  DEFINE STRUCTURE AgentResult:
    updates AS DICTIONARY // Changes to project_context (e.g., new ideas, evaluations)
    history_entry AS DICTIONARY // Record of the action taken and rationale
    status AS STRING // e.g., 'completed', 'failed', 'needs_review'
  END STRUCTURE

  // --- Specific Agent Implementations ---

  // 35.1 Programme Expert
  DEFINE STRUCTURE ProgrammeExpert EXTENDS Agent:
    role = "ProgrammeExpert"

    FUNCTION execute_task(task_details, project_context):
      // TDD_ANCHOR: test_expert_analyzes_criteria
      // TDD_ANCHOR: test_expert_evaluates_idea_fit
      // Goal: Evaluate ideas/drafts against Digital Europe Programme criteria. Provide feedback.
      LOG "ProgrammeExpert executing task: " + task_details.type
      DEFINE prompt = ""
      DEFINE result_updates = {}
      DEFINE history = {}

      IF task_details.type == "evaluate_idea":
        DEFINE idea = task_details.idea
        prompt = "Analyze this proposal idea: '" + idea.description + "' against the latest Digital Europe Programme funding criteria (focus on relevance, impact, innovation, consortium strength if applicable). Provide a structured evaluation (scores/comments) and suggest improvements for alignment."
        // Add relevant context from project_context if needed
        DEFINE evaluation = llm.generate(prompt)
        result_updates = { evaluations: [evaluation] } // Append new evaluation
        history = { agent: role, action: "evaluated_idea", idea_id: idea.id, evaluation: evaluation }
      ELSE IF task_details.type == "review_draft":
        DEFINE draft = task_details.draft
        prompt = "Review this proposal draft section: '" + draft.content + "' for compliance with Digital Europe Programme requirements and tone. Highlight weaknesses and suggest specific revisions."
        DEFINE review = llm.generate(prompt)
        result_updates = { draft_reviews: [review] } // Append new review
        history = { agent: role, action: "reviewed_draft", draft_id: draft.id, review: review }
      ELSE:
        LOG_ERROR "Unknown task type for ProgrammeExpert: " + task_details.type
        RETURN CREATE AgentResult(updates={}, history={agent: role, action: "error", details: "Unknown task"}, status="failed")
      ENDIF

      RETURN CREATE AgentResult(updates=result_updates, history_entry=history, status="completed")
    END FUNCTION
  END STRUCTURE

  // 35.2 Creative Ideator
  DEFINE STRUCTURE CreativeIdeator EXTENDS Agent:
    role = "CreativeIdeator"

    FUNCTION execute_task(task_details, project_context):
      // TDD_ANCHOR: test_ideator_generates_multiple_ideas
      // TDD_ANCHOR: test_ideator_uses_context
      // TDD_ANCHOR: test_ideator_uses_web_browsing (if enabled)
      // Goal: Generate innovative, viable proposal ideas based on context or initial user input.
      LOG "CreativeIdeator executing task: " + task_details.type
      DEFINE prompt = ""
      DEFINE result_updates = {}
      DEFINE history = {}

      IF task_details.type == "generate_initial_ideas":
        DEFINE initial_input = task_details.get("user_input", "")
        prompt = "Generate 5 distinct, innovative, and high-impact project ideas suitable for the EU Digital Europe Programme (€10M-€20M funding range). Focus on areas like AI, cybersecurity, digital skills, or green digital transition. Consider current EU priorities. "
        IF initial_input:
          prompt += "Use this initial direction: '" + initial_input + "'. "
        // Optionally add context about previous successful/failed ideas from project_context.history
        DEFINE ideas_text = llm.generate(prompt)
        DEFINE new_ideas = parse_ideas(ideas_text) // Function to structure LLM output
        // Optional: Use browser to quickly validate novelty/relevance of top ideas
        result_updates = { ideas: new_ideas } // Add new ideas
        history = { agent: role, action: "generated_initial_ideas", count: length(new_ideas) }
      ELSE IF task_details.type == "refine_idea":
         DEFINE idea_to_refine = task_details.idea
         DEFINE feedback = task_details.feedback // From Judge or Expert
         prompt = "Refine and expand this project idea: '" + idea_to_refine.description + "' based on the following feedback: '" + feedback + "'. Generate 2 improved variations."
         DEFINE refined_ideas_text = llm.generate(prompt)
         DEFINE refined_ideas = parse_ideas(refined_ideas_text)
         // Mark original idea as superseded? Or add variations? TBD by Workflow Engine.
         result_updates = { ideas: refined_ideas } // Add refined ideas
         history = { agent: role, action: "refined_idea", original_idea_id: idea_to_refine.id, feedback_used: feedback }
      ELSE:
        LOG_ERROR "Unknown task type for CreativeIdeator: " + task_details.type
        RETURN CREATE AgentResult(updates={}, history={agent: role, action: "error", details: "Unknown task"}, status="failed")
      ENDIF

      RETURN CREATE AgentResult(updates=result_updates, history_entry=history, status="completed")
    END FUNCTION

    FUNCTION parse_ideas(text_output):
      // TDD_ANCHOR: test_parse_ideas_structure
      // Logic to parse LLM text into a list of structured Idea objects
      DEFINE ideas_list = []
      // ... parsing logic ...
      RETURN ideas_list
    END FUNCTION

    DEFINE STRUCTURE Idea:
      id AS STRING // Unique ID
      description AS STRING
      keywords AS LIST<STRING>
      potential_impact AS STRING
      status AS STRING // e.g., 'new', 'evaluated', 'refined', 'discarded'
    END STRUCTURE
  END STRUCTURE

  // 35.3 Startup Founder
  DEFINE STRUCTURE StartupFounder EXTENDS Agent:
    role = "StartupFounder"

    FUNCTION execute_task(task_details, project_context):
      // TDD_ANCHOR: test_founder_evaluates_viability
      // TDD_ANCHOR: test_founder_considers_stakeholders
      // TDD_ANCHOR: test_founder_suggests_partners
      // Goal: Assess ideas for practical viability, market fit, stakeholder alignment, potential consortium.
      LOG "StartupFounder executing task: " + task_details.type
      DEFINE prompt = ""
      DEFINE result_updates = {}
      DEFINE history = {}

      IF task_details.type == "assess_viability":
        DEFINE idea = task_details.idea
        prompt = "Assess the practical viability and potential market impact of this EU project idea: '" + idea.description + "'. Consider: target stakeholders, potential challenges, required resources, and suggest potential types of consortium partners needed for success. Provide a structured assessment."
        // Add context: Programme Expert evaluation?
        DEFINE assessment = llm.generate(prompt)
        result_updates = { evaluations: [assessment] } // Append new evaluation (specific type: viability)
        history = { agent: role, action: "assessed_viability", idea_id: idea.id, assessment: assessment }
      ELSE:
        LOG_ERROR "Unknown task type for StartupFounder: " + task_details.type
        RETURN CREATE AgentResult(updates={}, history={agent: role, action: "error", details: "Unknown task"}, status="failed")
      ENDIF

      RETURN CREATE AgentResult(updates=result_updates, history_entry=history, status="completed")
    END FUNCTION
  END STRUCTURE

  // 35.4 Practical Judge
  DEFINE STRUCTURE PracticalJudge EXTENDS Agent:
    role = "PracticalJudge"

    FUNCTION execute_task(task_details, project_context):
      // TDD_ANCHOR: test_judge_critiques_idea
      // TDD_ANCHOR: test_judge_identifies_weaknesses
      // TDD_ANCHOR: test_judge_requests_refinement
      // Goal: Critically evaluate ideas/drafts, identify weaknesses, and suggest concrete improvements or rejection.
      LOG "PracticalJudge executing task: " + task_details.type
      DEFINE prompt = ""
      DEFINE result_updates = {}
      DEFINE history = {}

      IF task_details.type == "critique_idea":
        DEFINE idea = task_details.idea
        DEFINE evaluations = task_details.evaluations // From Expert, Founder
        prompt = "Critically judge this proposal idea: '" + idea.description + "' based on the following evaluations: " + format_evaluations(evaluations) + ". Identify the top 3 weaknesses and provide specific, actionable suggestions for improvement OR recommend discarding the idea with justification."
        DEFINE critique = llm.generate(prompt)
        result_updates = { evaluations: [critique] } // Append new evaluation (specific type: critique)
        history = { agent: role, action: "critiqued_idea", idea_id: idea.id, critique: critique }
        // Workflow engine might use this critique to trigger refinement or discard
      ELSE IF task_details.type == "review_draft_section":
        DEFINE draft_section = task_details.draft_section
        DEFINE reviews = task_details.reviews // From Expert
        prompt = "Critically review this draft section: '" + draft_section.content + "' based on prior reviews: " + format_reviews(reviews) + ". Is it compelling and clear? Identify flaws in logic, argumentation, or writing style. Provide concrete rewrite suggestions."
        DEFINE critique = llm.generate(prompt)
        result_updates = { draft_reviews: [critique] } // Append new review (specific type: critique)
        history = { agent: role, action: "critiqued_draft", draft_id: draft_section.id, critique: critique }
      ELSE:
        LOG_ERROR "Unknown task type for PracticalJudge: " + task_details.type
        RETURN CREATE AgentResult(updates={}, history={agent: role, action: "error", details: "Unknown task"}, status="failed")
      ENDIF

      RETURN CREATE AgentResult(updates=result_updates, history_entry=history, status="completed")
    END FUNCTION

    FUNCTION format_evaluations(evals): // Helper
      // TDD_ANCHOR: test_format_evaluations
      RETURN "Expert: " + evals.expert + "; Founder: " + evals.founder // Simplified
    END FUNCTION
    FUNCTION format_reviews(reviews): // Helper
      // TDD_ANCHOR: test_format_reviews
      RETURN "Expert Review: " + reviews.expert // Simplified
    END FUNCTION
  END STRUCTURE

  // 35.5 Proposal Writer
  DEFINE STRUCTURE ProposalWriter EXTENDS Agent:
    role = "ProposalWriter"

    FUNCTION execute_task(task_details, project_context):
      // TDD_ANCHOR: test_writer_drafts_section
      // TDD_ANCHOR: test_writer_incorporates_feedback
      // TDD_ANCHOR: test_writer_assembles_final_doc
      // Goal: Draft proposal sections based on refined ideas and feedback, assemble the final document.
      LOG "ProposalWriter executing task: " + task_details.type
      DEFINE prompt = ""
      DEFINE result_updates = {}
      DEFINE history = {}

      IF task_details.type == "draft_section":
        DEFINE idea = task_details.idea // The chosen, refined idea
        DEFINE section_name = task_details.section_name // e.g., "Introduction", "Methodology"
        DEFINE relevant_context = filter_context_for_section(project_context, idea.id, section_name)
        prompt = "Draft the '" + section_name + "' section of an EU Digital Europe Programme proposal based on this core idea: '" + idea.description + "' and relevant context: " + relevant_context + ". Ensure the writing is compelling, clear, and addresses typical requirements for this section."
        DEFINE draft_content = llm.generate(prompt)
        DEFINE new_draft = CREATE DraftSection(id=generate_id(), section=section_name, content=draft_content, idea_id=idea.id)
        result_updates = { drafts: [new_draft] } // Add new draft section
        history = { agent: role, action: "drafted_section", section: section_name, idea_id: idea.id }
      ELSE IF task_details.type == "revise_section":
        DEFINE draft_section = task_details.draft_section
        DEFINE critiques = task_details.critiques // From Judge, Expert
        prompt = "Revise this draft section: '" + draft_section.content + "' based on the following critiques: " + format_critiques(critiques) + ". Implement the suggested changes to strengthen the section."
        DEFINE revised_content = llm.generate(prompt)
        // Update existing draft or create new version? TBD by Workflow Engine. Assume update for now.
        draft_section.content = revised_content
        result_updates = { drafts: [draft_section] } // Indicate updated draft
        history = { agent: role, action: "revised_section", draft_id: draft_section.id, critiques_used: critiques }
      ELSE IF task_details.type == "assemble_final_proposal":
        DEFINE final_idea = task_details.idea
        DEFINE all_drafts = find_final_drafts_for_idea(project_context.drafts, final_idea.id)
        prompt = "Assemble the following approved proposal sections into a complete, coherent EU Digital Europe Programme application document in Markdown format. Ensure smooth transitions and consistent formatting. Sections: " + format_sections_for_assembly(all_drafts)
        DEFINE final_document_md = llm.generate(prompt)
        result_updates = { final_proposal: { idea_id: final_idea.id, content_md: final_document_md } }
        history = { agent: role, action: "assembled_final_proposal", idea_id: final_idea.id }
      ELSE:
        LOG_ERROR "Unknown task type for ProposalWriter: " + task_details.type
        RETURN CREATE AgentResult(updates={}, history={agent: role, action: "error", details: "Unknown task"}, status="failed")
      ENDIF

      RETURN CREATE AgentResult(updates=result_updates, history_entry=history, status="completed")
    END FUNCTION

    DEFINE STRUCTURE DraftSection:
      id AS STRING
      idea_id AS STRING
      section AS STRING // e.g., "Introduction", "Impact"
      content AS STRING
      version AS INTEGER
      status AS STRING // e.g., 'draft', 'reviewed', 'approved', 'rejected'
    END STRUCTURE

    FUNCTION filter_context_for_section(context, idea_id, section_name): // Helper
      // TDD_ANCHOR: test_filter_context
      // Logic to extract relevant evaluations, critiques for the specific idea/section
      RETURN "Filtered context string..."
    END FUNCTION
    FUNCTION format_critiques(critiques): // Helper
      // TDD_ANCHOR: test_format_critiques
      RETURN "Judge: " + critiques.judge + "; Expert: " + critiques.expert // Simplified
    END FUNCTION
    FUNCTION find_final_drafts_for_idea(all_drafts, idea_id): // Helper
      // TDD_ANCHOR: test_find_final_drafts
      // Logic to find the latest 'approved' version of each section for the idea
      RETURN [] // List of DraftSection objects
    END FUNCTION
    FUNCTION format_sections_for_assembly(drafts): // Helper
      // TDD_ANCHOR: test_format_sections_assembly
      // Logic to format section content for the final assembly prompt
      RETURN "Formatted sections string..."
    END FUNCTION

  END STRUCTURE

END MODULE